import torch
import gc
import sys
import os
import collections
import time

print("[DEBUG] 1. ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
time.sleep(0.1) # ì¶œë ¥ ë²„í¼ ë¹„ìš°ê¸°ìš©

# --- 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì • ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
try:
    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)
    print(f"[DEBUG] 2. PROJECT_ROOT ì„¤ì • ì™„ë£Œ: {PROJECT_ROOT}")
except Exception as e:
    print(f"[FATAL] PROJECT_ROOT ì„¤ì • ì‹¤íŒ¨: {e}")
    sys.exit(1)

# --- 2. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆ ì„í¬íŠ¸ ---
# from client_backend.model.base_llm import BaseLLMLoader
# from client_backend.model.preprocessing import LLMPreProcessor
# from client_backend.model.postprocessing import LLMPostProcessor
print("[DEBUG] 3. client_backend ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„...")
try:
    from client_backend.model.base_llm import BaseLLMLoader
    print("  - BaseLLMLoader ì„í¬íŠ¸ ì„±ê³µ")
    from client_backend.model.preprocessing import LLMPreProcessor
    print("  - LLMPreProcessor ì„í¬íŠ¸ ì„±ê³µ")
    from client_backend.model.postprocessing import LLMPostProcessor
    print("  - LLMPostProcessor ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"\n[FATAL] ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨! í´ë” ì´ë¦„ì´ 'client_backend'(ì–¸ë”ìŠ¤ì½”ì–´)ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    print(f"ì—ëŸ¬ ë©”ì‹œì§€: {e}")
    print(f"í˜„ì¬ sys.path: {sys.path}\n")
    sys.exit(1)
except Exception as e:
    print(f"[FATAL] ì˜ˆìƒì¹˜ ëª»í•œ ì„í¬íŠ¸ ì—ëŸ¬: {e}")
    sys.exit(1)

# --- 3. ì˜ì¡´ì„± ëª¨ë“ˆ ì„í¬íŠ¸ (ì„¤ì •ê°’) ---
# from common.config import (
#    MAX_GEN_LENGTH, DEVICE, HF_CACHE_DIR, 
#    LLM_NAME, BNB_COMPUTE_DTYPE, R_RANK, LORA_ALPHA, LORA_TARGET_MODULES,
#    REPRESENTATIVE_LORA_TARGET_MODULE, LORA_INJECTION_MODULES
#)
print("[DEBUG] 4. common ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„...")
try:
    import torch
    import gc
    from common.config import (
        MAX_GEN_LENGTH, DEVICE, HF_CACHE_DIR, 
        LLM_NAME, BNB_COMPUTE_DTYPE
    )
    print("  - common ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"[FATAL] common ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

def run_llm_test():
    """
    [LLM ë‹¨ë… í…ŒìŠ¤íŠ¸]
    FHE/ì„œë²„ ì—°ë™ ì—†ì´, LoRA í›… ì•„í‚¤í…ì²˜(base_llm, preprocessing, postprocessing)ê°€
    '0-ë¸íƒ€' ì£¼ì… ì‹œ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print("--- ğŸš€ LLM ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹œì‘ ---")
    gc.collect()
    torch.cuda.empty_cache()

    # 1. ëª¨ë“ˆ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ ëŒ€ìƒ)
    print("[Test] 1/3: LLM ë¡œë” (í›… í¬í•¨) ì´ˆê¸°í™” ì¤‘...")
    llm_loader = BaseLLMLoader()
    llm_loader.load_model()
    
    print("[Test] 2/3: ì „/í›„ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì¤‘...")
    preprocessor = LLMPreProcessor(llm_loader=llm_loader)
    postprocessor = LLMPostProcessor(llm_loader=llm_loader)
    
    print("[Test] 3/3: í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •...")
    prompt = "Write a Python function that returns the factorial of a number."
    generated_ids = []

    try:
        # --- 1. ì´ˆê¸° ìƒíƒœ ê°€ì ¸ì˜¤ê¸° ---
        print("\n--- [Test] Step 1: LLM ì´ˆê¸° ìƒíƒœ (xL í¬í•¨) ê°€ì ¸ì˜¤ê¸° ---")
        llm_loader.reset_lora_weights() # LoRA ê°€ì¤‘ì¹˜ 0ìœ¼ë¡œ ë¦¬ì…‹
        llm_states = preprocessor.get_initial_states(prompt)
        generated_ids.extend(llm_states["generated_ids"])
        
        current_llm_hidden_state = llm_states["current_llm_hidden_state"]
        xL_tensor = llm_states["lora_xL_input"] # (Batch, Hidden)

        # --- 2. í† í°ë³„ ìƒì„± ë£¨í”„ ---
        for i in range(MAX_GEN_LENGTH):
            print(f"\n--- [Test] Step 2.{i+1}: í† í° {i+1} ìƒì„± ---")
            
            # --- [0-ë¸íƒ€ ì‹œë®¬ë ˆì´ì…˜] ---
            # 'xL_tensor' (Batch, Hidden)ì™€ ë™ì¼í•œ shapeì˜ 0-í…ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            # ì´ê²ƒì´ 'FHE ë…¸ì´ì¦ˆê°€ ë‚€ 0-ë¸íƒ€' (dec_lora_output_delta)ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.
            
            print(f"  (3-5) [Sim] 0-ë¸íƒ€ ìƒì„± (Shape: {xL_tensor.shape})...")
            dummy_delta = torch.zeros_like(xL_tensor).to(DEVICE)
            
            # (5b) 0-ë¸íƒ€ë¥¼ í›…ì— ì£¼ì…í•˜ê¸° ìœ„í•´ ì „ì—­ ë³€ìˆ˜ì— ì„¤ì •
            llm_loader.set_global_lora_output_delta(dummy_delta)
            
            # --- [í´ë¼ì´ì–¸íŠ¸ ë¡œì§ ì‹¤í–‰] ---
            # (6) ë‹¤ìŒ í† í° ì˜ˆì¸¡
            print("  (6) [Client] ë‹¤ìŒ í† í° ì˜ˆì¸¡ (ë¸íƒ€ëŠ” ë‹¤ìŒ ìŠ¤í…ì— ì£¼ì…ë¨)...")
            next_token_id, next_token_char = postprocessor.integrate_lora_delta_and_predict_token(
                current_llm_hidden_state=current_llm_hidden_state
            )
            
            generated_ids.append(next_token_id)
            print(f"  -> ìƒì„±: {repr(next_token_char)}")

            if next_token_id == llm_loader.eos_token_id:
                print("\n  [Test] EOS í† í° ê°ì§€. ìƒì„± ì¢…ë£Œ.")
                break
            
            # (7) ìƒíƒœ ì—…ë°ì´íŠ¸ (ì´ë•Œ 'inject_delta_output_hook'ì´ 0-ë¸íƒ€ë¥¼ ì£¼ì…í•¨)
            print("  (7) [Client] ìƒíƒœ ì—…ë°ì´íŠ¸ (í›…ì„ í†µí•´ 0-ë¸íƒ€ ì£¼ì…)...")
            llm_states = preprocessor.get_next_token_states(next_token_id, llm_states)
            
            # (7b) ë‹¤ìŒ ë£¨í”„ë¥¼ ìœ„í•´ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
            llm_loader.clear_global_lora_output_delta() # ì£¼ì… ì™„ë£Œ í›„ ë¸íƒ€ ì´ˆê¸°í™”
            current_llm_hidden_state = llm_states["current_llm_hidden_state"]
            xL_tensor = llm_states["lora_xL_input"] # ìƒˆ xL

        # --- 3. ìµœì¢… í…ìŠ¤íŠ¸ ë””ì½”ë”© ---
        final_generated_text = postprocessor.decode_final_output(generated_ids)

        gc.collect()
        torch.cuda.empty_cache()
        
        print("\n" + "="*30)
        print("    âœ… ìµœì¢… ìƒì„± ê²°ê³¼ (LLM ë‹¨ë… í…ŒìŠ¤íŠ¸)")
        print("="*30)
        print(final_generated_text)
        print("="*30)

    except Exception as e:
        print(f"\n[Test] ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'llm_loader' in locals():
            llm_loader.clear_lora_hooks()
        print("\n--- ğŸ§¹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ---")

if __name__ == "__main__":
    run_llm_test()