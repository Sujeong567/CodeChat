"""
ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ 
LoRA A, B í–‰ë ¬ì„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¼
ì„¤ì • ì •ë³´(rank, alpha ë“± íŒŒì‹±)
"""

"""
LoRA ì–´ëŒ‘í„° ë¡œë”©
- adapter_model.bin, adapter_config.json ì½ê¸°
- LoRA A, B í–‰ë ¬ ë©”ëª¨ë¦¬ì— ë¡œë“œ
"""

import torch
import json
import os
from pathlib import Path


def load_lora_adapter(lora_path: str = None):
    """
    ë°ì´í„°íŒ€ì´ í•™ìŠµí•œ LoRA ê°€ì¤‘ì¹˜ ë¡œë”©
    
    Args:
        lora_path: LoRA í´ë” ê²½ë¡œ
    
    Returns:
        {
            'weights': LoRA ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬,
            'config': ì„¤ì • ì •ë³´,
            'rank': LoRA rank,
            'alpha': LoRA alpha
        }
    """
    
    if lora_path is None:
        lora_path = "./models/lora_weights/checkpoint-final"
    
    lora_path = Path(lora_path)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ LoRA ê²½ë¡œ: {lora_path}")
    print(f"{'='*60}\n")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    adapter_file = lora_path / "adapter_model.bin"
    config_file = lora_path / "adapter_config.json"
    
    if not adapter_file.exists():
        raise FileNotFoundError(f"adapter_model.bin ì—†ìŒ: {adapter_file}")
    
    if not config_file.exists():
        raise FileNotFoundError(f"adapter_config.json ì—†ìŒ: {config_file}")
    
    print("âœ… íŒŒì¼ ë°œê²¬!")
    print(f"   - adapter_model.bin: {adapter_file.stat().st_size / 1024 / 1024:.2f} MB")
    print(f"   - adapter_config.json: {config_file.stat().st_size / 1024:.2f} KB\n")
    
    # config ë¡œë“œ
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    rank = config.get("r", 4)
    alpha = config.get("lora_alpha", 32)
    
    print(f"   LoRA Rank: {rank}")
    print(f"   LoRA Alpha: {alpha}\n")
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    print("ğŸ“¦ LoRA ê°€ì¤‘ì¹˜ ë¡œë”© ì¤‘...")
    weights = torch.load(adapter_file, map_location="cpu")
    
    num_params = sum(p.numel() for p in weights.values())
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {num_params:,}\n")
    
    print(f"{'='*60}")
    print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ!")
    print(f"{'='*60}\n")

        # 5. ê°€ì¤‘ì¹˜ ë¶„ì„
    print("ğŸ“Š LoRA ê°€ì¤‘ì¹˜ ë¶„ì„:")
    
    num_params = sum(p.numel() for p in weights.values())
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {num_params:,}")
    
    # A, B í–‰ë ¬ ê°œìˆ˜ ì„¸ê¸°
    lora_A_count = sum(1 for k in weights.keys() if "lora_A" in k)
    lora_B_count = sum(1 for k in weights.keys() if "lora_B" in k)
    print(f"   LoRA A í–‰ë ¬ ê°œìˆ˜: {lora_A_count}")
    print(f"   LoRA B í–‰ë ¬ ê°œìˆ˜: {lora_B_count}")
    
    # ìƒ˜í”Œ í™•ì¸ (ì²« ë²ˆì§¸ ë ˆì´ì–´)
    first_key = list(weights.keys())[0]
    print(f"\n   ìƒ˜í”Œ í‚¤: {first_key}")
    print(f"   ìƒ˜í”Œ Shape: {weights[first_key].shape}")
    
    print(f"\n{'='*60}")
    print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ!")
    print(f"{'='*60}\n")
    
    return {
        'weights': weights,
        'config': config,
        'rank': rank,
        'alpha': alpha
    }


def extract_lora_matrices(weights: dict, layer_name: str):
    """
    íŠ¹ì • ë ˆì´ì–´ì˜ LoRA A, B í–‰ë ¬ ì¶”ì¶œ
    
    Args:
        weights: load_lora_adapter()ì—ì„œ ë°›ì€ ê°€ì¤‘ì¹˜
        layer_name: ë ˆì´ì–´ ì´ë¦„ (ì˜ˆ: "layers.0.self_attn.q_proj")
    
    Returns:
        (lora_A, lora_B) íŠœí”Œ
    """
    # í‚¤ íŒ¨í„´ ì°¾ê¸°
    lora_A_key = None
    lora_B_key = None
    
    for key in weights.keys():
        if layer_name in key:
            if "lora_A" in key:
                lora_A_key = key
            elif "lora_B" in key:
                lora_B_key = key
    
    if lora_A_key is None or lora_B_key is None:
        raise ValueError(f"Layer {layer_name}ì˜ LoRA í–‰ë ¬ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    lora_A = weights[lora_A_key]
    lora_B = weights[lora_B_key]
    
    print(f"ğŸ“ {layer_name}")
    print(f"   LoRA A shape: {lora_A.shape}")
    print(f"   LoRA B shape: {lora_B.shape}")
    
    return lora_A, lora_B
