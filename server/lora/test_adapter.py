import torch
import json
import os
from pathlib import Path
import sys
from adapter import load_lora_adapter, extract_lora_matrices

# load_lora_adapter, extract_lora_matrices í•¨ìˆ˜ëŠ” ìœ„ì— ì •ì˜ëœ ì›ë³¸ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# í¸ì˜ë¥¼ ìœ„í•´ ì—¬ê¸°ì— ë‹¤ì‹œ í¬í•¨í•˜ì§€ ì•Šê³ , ê°™ì€ íŒŒì¼ ë‚´ì— í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

# --- í•¨ìˆ˜ ì›ë³¸ (ì¬í™•ì¸ìš©) ---
# def load_lora_adapter(lora_path: str = None): ...
# def extract_lora_matrices(weights: dict, layer_name: str): ...
# -----------------------------

TEST_LORA_PATH = Path("./server/lora/lora_weights_checkpoints_final")
TEST_LAYER_NAME = "base_model.model.model.layers.0.self_attn.q_proj" 
# ë˜ëŠ” "layers.0.self_attn.q_proj" (ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ì‹¤ì œ í‚¤ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„)

def run_actual_lora_test():
    """ì‹¤ì œ ê²½ë¡œì—ì„œ LoRA íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("============================================================")
    print("ğŸš€ ì‹¤ì œ LoRA íŒŒì¼ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"âœ… ëŒ€ìƒ ê²½ë¡œ: {TEST_LORA_PATH.resolve()}")
    print("============================================================\n")

    try:
        # 1. load_lora_adapter í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        # ì‹¤ì œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—¬ê¸°ì„œ FileNotFoundError ë°œìƒ
        lora_data = load_lora_adapter(lora_path=str(TEST_LORA_PATH))
        
        weights = lora_data['weights']
        rank = lora_data['rank']
        alpha = lora_data['alpha']
        
        print("\n--- ë¡œë”© ê²°ê³¼ í™•ì¸ ---")
        print(f"ê°€ì ¸ì˜¨ LoRA Rank: {rank}")
        print(f"ê°€ì ¸ì˜¨ LoRA Alpha: {alpha}")
        print(f"Weights Key ê°œìˆ˜: {len(weights)}")

        # 2. extract_lora_matrices í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        # ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ì˜ ì‹¤ì œ í‚¤ êµ¬ì¡°ì— ë§ê²Œ TEST_LAYER_NAMEì„ ì¡°ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆ: PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” 'base_model.model.layers.0.self_attn.q_proj' ê°™ì€ ê¸´ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        # ê°€ì¥ í”í•œ LoRA í‚¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ë ˆì´ì–´ ì´ë¦„ì„ ì°¾ìŠµë‹ˆë‹¤.
        # ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ì— ì‹¤ì œë¡œ í¬í•¨ëœ í‚¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë ˆì´ì–´ ì´ë¦„ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
        potential_lora_keys = [k for k in weights.keys() if "lora_A" in k]
        
        if not potential_lora_keys:
             print("\nâŒ ì˜¤ë¥˜: ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ LoRA A/B í–‰ë ¬ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (lora_A í‚¤ ì—†ìŒ)")
             return
             
        # ì²« ë²ˆì§¸ LoRA A í‚¤ì—ì„œ ë ˆì´ì–´ ì´ë¦„(prefix) ì¶”ì¶œ
        # ì˜ˆ: 'base_model.model.layers.0.self_attn.q_proj.lora_A.default' -> 'base_model.model.layers.0.self_attn.q_proj'
        first_lora_key = potential_lora_keys[0]
        # lora_A.default ë˜ëŠ” lora_A ë¶€ë¶„ì„ ì œê±°
        test_layer_name_actual = first_lora_key.split(".lora_A")[0]
        
        print(f"\nâœ¨ í…ŒìŠ¤íŠ¸í•  ì‹¤ì œ ë ˆì´ì–´ ì´ë¦„: **{test_layer_name_actual}**")

        lora_A, lora_B = extract_lora_matrices(weights, test_layer_name_actual)
        
        # ì¶”ì¶œëœ í–‰ë ¬ Shape í™•ì¸ (rank ê°’ìœ¼ë¡œ ê²€ì¦)
        
        print("\nğŸ“ ìµœì¢… Shape ê²€ì¦:")
        print(f"  - LoRA A Shape: {lora_A.shape}")
        print(f"  - LoRA B Shape: {lora_B.shape}")
        
        # LoRA A í–‰ë ¬ì˜ ì²« ë²ˆì§¸ ì°¨ì›ê³¼ LoRA B í–‰ë ¬ì˜ ë‘ ë²ˆì§¸ ì°¨ì›ì´ rankì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
        if lora_A.shape[0] == rank and lora_B.shape[1] == rank:
            print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! LoRA í–‰ë ¬ì´ rankì™€ í•¨ê»˜ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ê²½ê³ : ì¶”ì¶œëœ í–‰ë ¬ì˜ Shapeì´ ì˜ˆìƒ(rank)ê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")

    except FileNotFoundError as e:
        print("\n============================================================")
        print("ğŸš¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: íŒŒì¼ì´ ì§€ì •ëœ ê²½ë¡œì— ì—†ìŠµë‹ˆë‹¤.")
        print(f"    {e}")
        print(f"    ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”: {TEST_LORA_PATH.resolve()}")
        print("============================================================")
        sys.exit(1)
    except RuntimeError as e:
        print("\n============================================================")
        print("ğŸš¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë”© ì˜¤ë¥˜")
        print(f"    PyTorch ë¡œë”© ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("    **safetensors íŒŒì¼ì„ torch.loadë¡œ ë¡œë“œí•  ë•Œ í˜•ì‹ì´ ë§ì§€ ì•Šì•„ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**")
        print("    adapter_model.safetensors íŒŒì¼ì´ ì‹¤ì œë¡œ PyTorch .bin í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        print("============================================================")
        sys.exit(1)
    except ValueError as e:
        print(f"\nğŸš¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì¶”ì¶œ ì˜¤ë¥˜ - {e}")
        print("    ì¶”ì¶œí•˜ë ¤ëŠ” ë ˆì´ì–´ ì´ë¦„ì´ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ í‚¤ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_actual_lora_test()