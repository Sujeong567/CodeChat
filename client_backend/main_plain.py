# client_backend/main_plain.py (Plaintext Baseline Version)
import os
import sys
import time
import gc
import requests
import torch
from fastapi import FastAPI, HTTPException
from contextlib import asynccontextmanager
from pydantic import __version__ as pydantic_version
import uvicorn
from typing import List

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from common.config import (
    CLIENT_BACKEND_HOST,
    CLIENT_BACKEND_PORT,
    SERVER_HOST,
    SERVER_PORT,
    DEVICE,
)
from common.protocol import (
    ClientBackendRequest,
    ClientBackendResponse,
    # ðŸš« HE/ì•”í˜¸í™” ê´€ë ¨ ìž„í¬íŠ¸ ì œê±°
    # EncryptedInferenceRequest,
    # EncryptedInferenceResponse,
    # encode_bytes_to_base64,
    # decode_base64_to_bytes,
    # âœ… Plaintext í”„ë¡œí† ì½œ ìž„í¬íŠ¸
    PlaintextInferenceRequest,
    PlaintextInferenceResponse,
)
# from client_backend.crypto.ckks_client import CKKSClientManager # ðŸš« ì œê±°
from client_backend.model.base_llm import BaseLLMLoader
from client_backend.model.preprocessing import LLMPreProcessor
from client_backend.model.postprocessing import LLMPostProcessor

PYDANTIC_V2 = pydantic_version.startswith("2.")
def model_dump(model):
    return model.model_dump() if PYDANTIC_V2 else model.dict()
def model_validate(model_cls, data):
    # PlaintextInferenceResponse, PlaintextInferenceRequest ë“±ì„ ìœ„í•´ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
    return model_cls.model_validate(data) if PYDANTIC_V2 else model_cls.parse_obj(data)

app_state = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("[ClientBackend] í‰ë¬¸ ì„œë²„ ì‹œìž‘ - ëª¨ë¸ ë¡œë“œ")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    loader = BaseLLMLoader()
    loader.load_model()

    app_state["llm_loader"] = loader
    app_state["preprocessor"] = LLMPreProcessor(loader)
    app_state["postprocessor"] = LLMPostProcessor(loader)
    # app_state["ckks_manager"] = CKKSClientManager() # ðŸš« ì œê±°

    app_state["http_session"] = requests.Session()
    app_state["server_url"] = f"http://{SERVER_HOST}:{SERVER_PORT}/compute_lora"

    print("[ClientBackend] ì´ˆê¸°í™” ì™„ë£Œ")
    yield

    app_state["http_session"].close()
    loader.clear_lora_hooks()
    print("[ClientBackend] ì„œë²„ ì¢…ë£Œ")

app = FastAPI(lifespan=lifespan)

@app.post("/generate", response_model=ClientBackendResponse)
async def generate(request: ClientBackendRequest):
    start_time = time.time()
    try:
        print(f"[ClientBackend] ì¶”ë¡  ìš”ì²­: '{request.prompt[:80]}' ...")

        loader: BaseLLMLoader = app_state["llm_loader"]
        preproc: LLMPreProcessor = app_state["preprocessor"]
        postproc: LLMPostProcessor = app_state["postprocessor"]
        # ckks: CKKSClientManager = app_state["ckks_manager"] # ðŸš« ì œê±°
        session: requests.Session = app_state["http_session"]
        server_url: str = app_state["server_url"]

        # ë§¤ ìš”ì²­ë§ˆë‹¤ LoRA ê°€ì¤‘ì¹˜ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        loader.reset_lora_weights()

        # 1) ì´ˆê¸° ìƒíƒœ
        states = preproc.get_initial_states(request.prompt)
        generated_ids = states["generated_ids"][:]

        max_steps = request.max_new_tokens

        for step in range(max_steps):
            print(f"[ClientBackend] Token step {step + 1}/{max_steps}")

            # 2) í˜„ìž¬ xL (1, hidden) -> (hidden,) -> í‰ë¬¸í™”
            xL = states["lora_xL_input"]  # (1, H)
            xL_vec = xL.squeeze(0)        # (H,)
            
            # ðŸš« ì•”í˜¸í™” ì œê±°
            # enc_bytes = ckks.encrypt_tensor(xL_vec)
            # âœ… í‰ë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            hidden_state_vec: List[float] = xL_vec.tolist()


            # 3) ì„œë²„ë¡œ ì „ì†¡ (Plaintext í”„ë¡œí† ì½œ ì‚¬ìš©)
            req_obj = PlaintextInferenceRequest(
                hidden_state_vec=hidden_state_vec
            )
            res = session.post(server_url, json=model_dump(req_obj))
            res.raise_for_status()
            
            # âœ… ì‘ë‹µ ëª¨ë¸ ë³€ê²½
            resp_obj = model_validate(PlaintextInferenceResponse, res.json())

            # 4) ì„œë²„ì—ì„œ ê³„ì‚°í•œ LoRA delta ì²˜ë¦¬ (ë³µí˜¸í™” ì œê±°)
            # ðŸš« ë³µí˜¸í™” ì œê±°
            # delta_bytes = decode_base64_to_bytes(resp_obj.enc_lora_delta_bytes)
            # delta_vec = ckks.decrypt_tensor(delta_bytes)  # (H,)
            
            # âœ… Plaintext ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
            delta_vec_list: List[float] = resp_obj.lora_delta_vec
            delta_vec = torch.tensor(delta_vec_list, dtype=torch.float32).to(DEVICE)
            
            # (H,) -> (1, H)
            delta_tensor = delta_vec.unsqueeze(0) 

            # 5) ë¸íƒ€ë¥¼ ì „ì—­ì— ì„¤ì • (hookì´ ì‚¬ìš©)
            loader.set_global_lora_output_delta(delta_tensor)


            # 6) í˜„ìž¬ hidden state ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í° argmax
            next_token_id, next_token_char = postproc.integrate_lora_delta_and_predict_token(
                states["current_llm_hidden_state"]
            )
            generated_ids.append(next_token_id)

            print(f"  -> ìƒì„± í† í°: {repr(next_token_char)}")

            if next_token_id == loader.eos_token_id:
                print("  EOS í† í° ê°ì§€, ì¢…ë£Œ.")
                break

            # 7) LLM ìƒíƒœ ì—…ë°ì´íŠ¸ (ì´ë•Œ hookì´ delta ì£¼ìž…í•˜ê³  ìƒˆ xL ìº¡ì²˜)
            states = preproc.get_next_token_states(next_token_id, states)

            # 8) ë¸íƒ€ ì£¼ìž… ì™„ë£Œ í›„ ì „ì—­ delta ì´ˆê¸°í™”
            loader.clear_global_lora_output_delta()

        final_text = postproc.decode_final_output(generated_ids)
        elapsed = time.time() - start_time

        print("[ClientBackend] ìµœì¢… ê²°ê³¼:")
        print(final_text[:500])
        print(f"[ClientBackend] ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")

        return ClientBackendResponse(
            generated_text=final_text,
            status="success",
            message=f"LLM ì¶”ë¡  ì™„ë£Œ ({elapsed:.2f}ì´ˆ)",
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    # âš ï¸ ì‹¤í–‰ ì‹œ ëª¨ë“ˆ ì´ë¦„ ë³€ê²½ (main -> main_plain)
    uvicorn.run(
        "client_backend.main_plain:app",
        host=CLIENT_BACKEND_HOST,
        port=CLIENT_BACKEND_PORT,
        reload=True,
    )